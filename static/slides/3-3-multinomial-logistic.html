<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>IDS 702: Module 3.3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# IDS 702: Module 3.3
## Multinomial logistic regression
### Dr. Olanrewaju Michael Akande

---








## Recall logistic regression

- Recall that for logistic regression, we had 
.block[
.small[
$$
y_i | x_i \sim \textrm{Bernoulli}(\pi_i); \ \ \ \textrm{log}\left(\dfrac{\pi_i}{1-\pi_i}\right) = \beta_0 + \beta_1 x_i
$$
]
]

  for each observation `\(i = 1, \ldots, n\)`.

--
 
- To get `\(\pi_i\)`, we solved the logit equation above to get
.block[
.small[
`$$\pi_i = \dfrac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}$$`
]
]

--

- Consider `\(Y=0\)` a baseline category. Suppose `\(\Pr[y_i = 1 | x_i] = \pi_{i1}\)` and `\(\Pr[y_i = 0 | x_i] = \pi_{i0}\)`. Then, the logit expression is essentially
.block[
.small[
`$$\textrm{log}\left(\dfrac{\pi_{i1}}{\pi_{i0}}\right) = \beta_0 + \beta_1 x_i$$`
]
]

--

- `\(e^{\beta_1}\)` is thus the (multiplicative) change in odds of `\(y = 1\)` over the baseline `\(y = 0\)` when increasing `\(x\)` by one unit.



---
## Multinomial logistic regression

- Suppose we have a nominal-scale response variable `\(Y\)` with `\(J\)` categories. First, for the .hlight[random component], we need a distribution to describe `\(Y\)`.

--

- A standard option for this is the .hlight[multinomial distribution], which is essentially a generalization of the binomial distribution.  
  Read about the multinomial distribution [here](https://www.olanrewajuakande.com/STA111-Summer2018-Course-Website/Lectures/Lecture6.pdf) and [here](https://en.wikipedia.org/wiki/Multinomial_distribution).

--

- .hlight[Multinomial distribution] gives us a way to characterize
.block[
.small[
`$$\Pr[y_i = 1] = \pi_1, \ Pr[y_i = 2] = \pi_2, \ \ldots, \ \Pr[y_i = J] = \pi_J, \ \ \ \textrm{where} \ \ \ \sum^J_{j=1} \pi_j = 1.$$`
]
]

--

- When there are no predictors, the best guess for each `\(\pi_j\)` is the sample proportion of cases with `\(y_i = j\)`, that is,
.block[
.small[
`$$\hat{\pi}_j = \dfrac{\mathbf{1}[y_i = j]}{n}$$`
]
]

--
 
- When we have predictors, then we want
.block[
.small[
`$$\Pr[y_i = 1 | \boldsymbol{x}_i] = \pi_{i1}, \ \Pr[y_i = 2 | \boldsymbol{x}_i] = \pi_{i2}, \ \ldots, \ \Pr[y_i = J | \boldsymbol{x}_i] = \pi_{iJ}.$$`
]
]



---
## Multinomial logistic regression

- That is, we want the `\(\pi_j\)`'s to be functions of the predictors, like in logistic regression.

--

- Turns out we can use the same .hlight[link function], that is the logit function, if we set one of the levels as the baseline.

--

- Pick a baseline outcome level, say `\(Y=1\)`.

--

- Then, the multinomial logistic regression is defined as a set of logistic regression models for each probability `\(\pi_j\)`, compared to the baseline, where `\(j\geq 2\)`. That is,
.block[
.small[
`$$\textrm{log}\left(\dfrac{\pi_{ij}}{\pi_{i1}}\right) = \beta_{0j} + \beta_{1j} x_{i1} + \beta_{2j} x_{i2} + \ldots + \beta_{pj} x_{ip},$$`
]
]

  where `\(j\geq 2\)`.

--
  
- We therefore have `\(J-1\)` .hlight[separate logistic regressions] in this setup.



---
## Multinomial logistic regression

- The equation for each `\(\pi_{ij}\)` is given by
.block[
.small[
`$$\pi_{ij} = \dfrac{e^{\beta_{0j} + \beta_{1j} x_{i1} + \beta_{2j} x_{i2} + \ldots + \beta_{pj} x_{ip}}}{1 + \sum^J_{j=2} e^{\beta_{0j} + \beta_{1j} x_{i1} + \beta_{2j} x_{i2} + \ldots + \beta_{pj} x_{ip}}} \ \ \ \textrm{for} \ \ \ j &gt; 1$$`
]
]

  and
.block[
.small[
`$$\pi_{i1} = 1-\sum^J_{j=2} \pi_{ij}$$`
]
]

--

- Also, we can extract the log odds for comparing other pairs of the response categories `\(j\)` and `\(j^\star\)`, since
.block[
.small[
$$
`\begin{split}
\textrm{log}\left(\dfrac{\pi_{ij}}{\pi_{ij^\star}}\right) &amp; = \textrm{log}\left(\pi_{ij}\right) - \textrm{log}\left(\pi_{ij^\star}\right) \\
 &amp; = \textrm{log}\left(\pi_{ij}\right) - \textrm{log}\left(\pi_{i1}\right) - \textrm{log}\left(\pi_{ij^\star}\right) + \textrm{log}\left(\pi_{i1}\right) \\
&amp; = \left[ \textrm{log}\left(\pi_{ij}\right) - \textrm{log}\left(\pi_{i1}\right) \right] - \left[ \textrm{log}\left(\pi_{ij^\star}\right) - \textrm{log}\left(\pi_{i1}\right) \right] \\
&amp; = \textrm{log}\left(\dfrac{\pi_{ij}}{\pi_{i1}}\right) - \textrm{log}\left(\dfrac{\pi_{ij^\star}}{\pi_{i1}}\right).
\end{split}`
$$
]
]


---
## Multinomial logistic regression

- Each coefficient has to be interpreted relative to the baseline.

--

- That is, for a continuous predictor,
  + `\(\beta_{1j}\)` is the .hlight[increase (or decrease) in the log-odds] of `\(Y=j\)` versus `\(Y=1\)` when increasing `\(x_1\)` by one unit.
  + `\(e^{\beta_{1j}}\)` is the .hlight[multiplicative increase (or decrease) in the odds] of `\(Y=j\)` versus `\(Y=1\)` when increasing `\(x_1\)` by one unit.

--
 
- Whereas, for a binary predictor,
  + `\(\beta_{1j}\)` is the .hlight[log-odds] of `\(Y=j\)` versus `\(Y=1\)` for the group with `\(x_1 = 1\)`, compared to the group with `\(x_1 = 0\)`.
  + `\(e^{\beta_{1j}}\)` is the .hlight[odds] of `\(Y=j\)` versus `\(Y=1\)` for the group with `\(x_1 = 1\)`, compared to the group with `\(x_1 = 0\)`.

--
 
- Exponentiate confidence intervals from log-odds scale to get on the odds scale.



---
## Significance tests

- For multinomial logistic regression, use the change in deviance test to compare models and test significance, just like we had for logistic regression.

--

- Fit model with and without some predictor `\(x_k\)`.

--

- Perform a change in deviance test to compare the two models.

--

- Interpret p-value as evidence about whether the coefficients excluded from the smaller model are equal to zero.



---
## Model diagnostics

- Use binned residuals like in logistic regression.

--

- Each outcome level has its own raw residual. For each outcome level `\(j\)`,

--

  + make an indicator variable equal to one whenever `\(Y = j\)` and equal to zero otherwise
  
--

  + compute the predicted probability that `\(Y=j\)` for each record (using the `fitted` command)

--

  + compute the raw residual = indicator value - predicted probability

--

- For each outcome level, make bins of predictor values and plot average value of predictor versus the average raw residual. Look for patterns.

--

- We can still compute .hlight[accuracy] just like we did for the logistic regression.

--

- ROC on the other hand is not so straightforward; we can draw a different ROC curve for each level of the response variable. We can also draw pairwise ROC curves.



---
## Implementation in R

- Install the package .hlight[nnet] from CRAN.

- Load the library: `library(nnet)`.

- The command for running the multinomial logistic regression in R looks like:
  
  ```r
  Modelfit &lt;- multinom (response ~ x_1 + x_2 + ... + x_p, data = Data)
  ```
  
- Use `fitted(Modelfit)` to get predicted probabilities for observed cases.

--

- We will see an example in the next module.



---

class: center, middle

# What's next? 

### Move on to the readings for the next module!




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
