<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>IDS 702: Module 1.2</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# IDS 702: Module 1.2
## Introduction to multiple linear regression
### Dr. Olanrewaju Michael Akande

---








## Multiple linear regression

- Multiple linear regression (MLR) assumes the following distribution for a response variable `\(y_i\)` given `\(p\)` potential covariates/predictors/features `\(\boldsymbol{x}_i = (x_{i1}, x_{i2}, \ldots, x_{ip})\)`.
.block[
.small[
`$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i; \ \ \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2), \ \ \ i = 1,\ldots, n.$$`
]
]

--

- We can also write the model as:
.block[
.small[
`$$y_i \overset{iid}{\sim} \mathcal{N}(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}, \sigma^2).$$`
`$$p(y_i | \boldsymbol{x}_i) = \mathcal{N}(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}, \sigma^2).$$`
]
]

--

- MLR assumes that the conditional average or expected value of a response variable is a linear function of potential predictors.

--

- Note that the linearity is in terms of the "unknown" parameters (intercept and slopes).

--

- Just like in SLR, MLR also assumes values of the response variable follow a normal curve within any combination of predictors.


---
## MLR

- Just as we had under SLR, here each `\(\beta_j\)` represents the true "unknown" value of the parameter, while `\(\hat{\beta}_j\)` represents the estimate of `\(\beta_j\)`.

--

- Similarly, `\(y_i\)` represents the true value of the response variable, while `\(\hat{y}_i\)` represents the predicted value. That is, 
.block[
.small[
`$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \hat{\beta} x_{i2} + \ldots + \hat{\beta} x_{ip}.$$`
]
]

--

- Also, the residuals `\(e_i\)` are our estimates of the true "unobserved" errors `\(\epsilon_i\)`. Thus, 
.block[
.small[
`$$e_i = y_i - \left[\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \hat{\beta} x_{i2} + \ldots + \hat{\beta} x_{ip}\right] = y_i - \hat{y}_i.$$`
]
]

--

- Since the `\(e_i\)`'s estimate the `\(\epsilon_i\)`'s, we expect them to also be .hlight[independent, centered at zero, and have constant variance].

--

- We will get into this more under model assessment. 


---
## MLR: estimation

- Estimated coefficients are found by taking partial derivatives of the sum of squares of the errors
.small[
`$$\sum^n_{i=1} \left(y_i - \left[\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} \right] \right)^2,$$`
]

  with respect to each parameter, that is, `\(\beta_0, \beta_1, \ldots, \beta_p\)`.
  
--

- This is the ordinary least squares (OLS) method.

--

- Resulting formulas are a bit messy to write down in this form.

--

- However, there is a very nice matrix algebra representation as we will see soon.



---
## MLR: estimation

- An alternative derivation uses maximum likelihood estimation (MLE).

--

- First, not that if each `\(Y_i\)`, with `\(i=1,\ldots,n\)`, follows the .hlight[normal distribution] `\(Y_i \sim \mathcal{N}(\mu, \sigma^2)\)`, then the likelihood is
.block[
.small[
$$
`\begin{split}
L(\mu,\sigma^2 | y_1,\ldots, y_n) &amp; = \prod_{i=1}^n \left( 2\pi\sigma^2 \right)^{-\frac{1}{2}}\ e^{-\frac{1}{2\sigma^2} \left(y_i-\mu\right)^2}\\
&amp; = \left( 2\pi\sigma^2 \right)^{-\frac{n}{2}}\ e^{-\frac{1}{2\sigma^2} \sum\limits_{i=1}^n \left(y_i-\mu\right)^2}.
\end{split}`
$$
]
]

--

- So that for MLR, the likelihood is
.block[
.small[
$$
`\begin{split}
L(\beta_0, \beta_1, \ldots, \beta_p,\sigma^2 | y_1,\ldots, y_n) &amp; = \left( 2\pi\sigma^2 \right)^{-\frac{n}{2}}\ e^{-\frac{1}{2\sigma^2} \sum\limits_{i=1}^n \left(y_i-\left[\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}\right]\right)^2}.
\end{split}`
$$
]
]

--

- To get the MLEs, take the log of the likelihood, differentiate with respect to each parameter in `\((\beta_0, \beta_1, \ldots, \beta_p,\sigma^2)\)`, and set to zero.

--

- Again, resulting formulas for `\((\beta_0, \beta_1, \ldots, \beta_p)\)` are a bit messy to write down in this form.



---
## MLR: estimation

- The MLE for `\(\sigma^2\)` (work it out to convince yourself) is
.block[
.small[
$$
`\begin{split}
\hat{\sigma}^2_{\text{MLE}} &amp;= \frac{1}{n} \sum^n_{i=1} \left(y_i - \left[\hat{\beta}_0 + \hat{\beta_1} x_{i1} + \ldots + \hat{\beta_p} x_{ip}\right] \right)^2\\
&amp;= \frac{1}{n} \sum^n_{i=1} \left(y_i - \hat{y}_i \right)^2 = \frac{1}{n} \sum^n_{i=1} e_i^2.
\end{split}`
$$
]
]

--

- However, the MLE is biased. That is, `\(\mathbb{E}[\hat{\sigma}^2_{\text{MLE}}] \neq \sigma^2\)`.

--

- Therefore, we often used the following "unbiased" estimator for `\(\sigma^2\)`.
.block[
.small[
`$$\hat{\sigma}^2 = s_e^2 = \frac{1}{n-(p+1)} \sum^n_{i=1} \left(y_i - \hat{y}_i \right)^2 = \frac{1}{n-(p+1)} \sum^n_{i=1} e_i^2.$$`
]
]

--

- Most software packages will estimate `\(s_e^2\)` automatically.



---
## MLR: matrix representation

- Let
.midsmall[
$$
\boldsymbol{y} =
`\begin{bmatrix}
y_1 \\
y_2 \\
\vdots\\
y_n \\
\end{bmatrix}`
\hspace{0.5em}
\boldsymbol{X} =
`\begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1p} \\
1 &amp; x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \ldots &amp; x_{np} \\
\end{bmatrix}`
\hspace{0.5em}
\boldsymbol{\beta} =
`\begin{bmatrix}
\beta_0\\
\beta_1\\
\beta_2 \\
\vdots \\
\beta_p \\
\end{bmatrix}`
\hspace{0.5em}
\boldsymbol{\epsilon} =
`\begin{bmatrix}
\epsilon_1\\
\epsilon_2 \\
\vdots \\
\epsilon_n \\
\end{bmatrix}`
\hspace{0.5em}
\boldsymbol{I} =
`\begin{bmatrix}
1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; 1 \\
\end{bmatrix}`
$$ 
]

--

- Then, we can write the MLR model as
.block[
.small[
`$$\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}; \ \ \boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2 \boldsymbol{I}).$$`
]
]

--

- The OLS and MLE estimates of all `\((p+1)\)` coefficients (intercept plus `\(p\)` slopes) is then given by
.block[
.small[
`$$\hat{\boldsymbol{\beta}} = \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T \boldsymbol{y}.$$`
]
]

--
&lt;div class="question"&gt;
Ideally, n should be bigger than p. Why?
&lt;/div&gt;

--

  There are many ways around the `\(p &gt; n\)` problem. If there is time, we may look at some options.


---
## MLR: matrix representation

- The predictions can then be written as
.block[
.small[
`$$\hat{\boldsymbol{y}} =  \boldsymbol{X}\hat{\boldsymbol{\beta}} = \boldsymbol{X} \left[\left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T \boldsymbol{y} \right] = \left[\boldsymbol{X} \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T \right] \boldsymbol{y}.$$`
]
]

--

- The residuals can be written as
.block[
.small[
`$$\boldsymbol{e} = \boldsymbol{y} - \hat{\boldsymbol{y}} = \boldsymbol{y} - \left[\boldsymbol{X} \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T \right] \boldsymbol{y} =  \left[\boldsymbol{1}_n - \boldsymbol{X} \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T \right] \boldsymbol{y}$$`
]
]
where `\(\boldsymbol{1}_n\)` is a matrix of ones

--

- The `\(n \times n\)` matrix
.block[
.small[
`$$\boldsymbol{H} = \boldsymbol{X} \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T$$`
]
]
is often called the .hlight[projection matrix] or the .hlight[hat matrix].

--

- We will see some important features of the elements of `\(\boldsymbol{H}\)` soon.


---
## MLR: matrix representation

- In matrix form,
.block[
.small[
`$$s_e^2  = \sum^n_{i=1} \dfrac{\left(y_i - \hat{y}_i \right)^2}{n-(p+1)} = \dfrac{(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})^T(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}})}{n-(p+1)} = \dfrac{\boldsymbol{e}^T\boldsymbol{e}}{n-(p+1)}.$$`
]
]

--

- The variance of the OLS estimates of all `\((p+1)\)` coefficients (intercept plus `\(p\)` slopes) is
.block[
.small[
$$\mathbb{V}\left[ \hat{\boldsymbol{\beta}} \right] = \sigma^2 \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} $$
]
]

--

- Notice that this is a covariance matrix; the square root of the diagonal elements give us the standard errors for each `\(\beta_j\)`, which we can use for hypothesis testing and interval estimation.
--
&lt;div class="question"&gt;
What are the off-diagonal elements?
&lt;/div&gt;

--

- When estimating `\(\mathbb{V}[\hat{\boldsymbol{\beta}}]\)`, plug in `\(s_e^2\)` as an estimate of `\(\sigma^2\)`.

--

- Now that we have a basic introduction, we are ready see how to fit MLR models.



---

class: center, middle

# What's next? 

### Move on to the readings for the next module!




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
