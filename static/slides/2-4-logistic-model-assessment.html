<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>IDS 702: Module 2.4</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# IDS 702: Module 2.4
## Model assessment and validation - binned residuals and roc curves
### Dr. Olanrewaju Michael Akande

---








## Model assessment and validation

There are various types of residuals when working with generalized linear models (GLMs). For logistic regression in particular, we have

- .hlight[Response residuals]
  .block[
.small[
`$$e_i = y_i - \hat{\pi}_i.$$`
]
]

--

- .hlight[Pearson residuals]
  .block[
.small[
`$$e_i^P = \dfrac{y_i - \hat{\pi}_i}{\sqrt{\hat{\pi}_i(1-\hat{\pi}_i)}},$$`
]
]

  which are obtained by "normalizing" the response residuals by the estimated Bernoulli standard deviation.

--

- .hlight[Deviance residuals]
  .block[
.small[
`$$e_i^D = \textrm{sign}(y_i - \hat{\pi}_i) \times 2\left(y_i \textrm{log}\dfrac{1}{\hat{\pi}_i} + (1-y_i) \textrm{log}\dfrac{1}{1-\hat{\pi}_i} \right),$$`
]
]
 
 which are the default in R when using the .hlight[residuals()] function. We will talk a bit more about deviance later, but deviance residuals represent the contributions of individual samples to the deviance.

 
 

---
## Model assessment and validation

- Deviance residuals are usually the most appropriate for residual plots, when working with GLMs.

--

- However, unlike what we had for linear regression, just looking at the residuals does not work well here.
  + They are always positive when `\(Y=1\)` and always negative when `\(Y=0\)`.
  
--

  + Also, constant variance is not an assumption of logistic regression.
  
     &lt;div class="question"&gt;
    Why is that the case?
    &lt;/div&gt;  
    Think about the properties of the Bernoulli distribution when we write `\(y_i | x_i \sim \textrm{Bernoulli}(\pi_i)\)`
     
--

  + We also do not have normality of residuals to work with either.



---
## Model assessment and validation

- What we can do is check to see if the function of predictors is well specified using .hlight[binned residuals].

--

- We can assess the overall fit of our model using .hlight[deviance] and .hlight[change in deviance].

--

- We can also see how well our model predicts (model validation) using
 + Confusion matrix

 + ROC curves
 


---
## Binned residuals

- Compute raw (response) residuals for fitted logistic regression.

- Order observations by values of predicted probabilities (or predictor values) from the fitted regression.

- Using ordered data, form `\(g\)` bins of (approximately) equal size. Default: `\(g = \sqrt{n}\)`.

- Compute average residual in each bin.

- Plot average residual versus average predicted probability (or average predictor value) for each bin.

- Use the .hlight[arm] package in R.


---
## NBA analysis

Recall the NBA data


```r
nba &lt;- read.csv("data/nba_games_stats_reduced.csv",header=T,
                stringsAsFactors=T)
nba &lt;- nba[nba$Team=="SAS",]
colnames(nba)[3] &lt;- "Opp"
nba$win &lt;- rep(0,nrow(nba))
nba$win[nba$WINorLOSS=="W"] &lt;- 1
nba$win &lt;- as.factor(nba$win)
nba$Opp_cent &lt;- nba$Opp - mean(nba$Opp)
nbareg &lt;- glm(win~Opp_cent,family=binomial(link=logit),data=nba)
```



---
## NBA analysis


```r
plot(nbareg,which=1)
```

&lt;img src="2-4-logistic-model-assessment_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

The residuals are the deviance residuals, while the predicted values are on the linear (logit) scale, that is, `\(\beta_0 + \beta_1 x_i\)`.

Look to see which cases have large absolute values for cases that don't fit well, but not too useful otherwise.



---
## NBA analysis

Plot binned raw residuals versus predicted probabilities (.hlight[arm] package).



```r
binnedplot(fitted(nbareg),residuals(nbareg,"resp"),xlab="Pred. probabilities",col.int="red4",
           ylab="Avg. residuals",main="Binned residual plot",col.pts="navy")
```

&lt;img src="2-4-logistic-model-assessment_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

Look for "randomness" with almost all points within the red lines.


---
## NBA analysis

- Useful as a "one-stop shopping" plot; especially with many predictors and you want an initial look at model adequacy. 

--

- What we have is mostly good, although model seems to struggle for fitted values over 0.95 or so.

--

- The red lines represent `\(\pm 2\)` SE bands, which we would expect to contain about 95% of the observations. 

--

- Too few points here to draw any conclusions!

--

- You usually want many more data points before these plots start being useful.


---
## NBA analysis

Plot binned raw residuals versus individual predictors.


```r
binnedplot(nba$Opp,residuals(nbareg,"resp"),xlab="Opponent's points (centered)",
           col.int="red4",ylab="Avg. residuals",main="Binned residual plot",col.pts="navy")
```

&lt;img src="2-4-logistic-model-assessment_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;



---
## NBA analysis

- Mostly good, although model seems to struggle for low values of opponent's points.

--

- Also, too many points (16.7%) outside the bands.

--

- However, still too few points here for any conclusive takeaways.

--

- We also know some important predictors are missing by construction...




---
## Deviance

- To assess overall model fit, we can also look at .hlight[deviance].

--

- Deviance measures how well the model fits the data, when compared to the .hlight[saturated model], that is, an abstract model that fits the sample perfectly.

--

- Precisely, deviance is defined as the difference of likelihoods between the fitted model and the saturated model:
.block[
.small[
`$$D = - 2 \left[ \text{ Log Likelihood}(\text{Fitted Model}) - \text{ Log Likelihood}(\text{Saturated Model}) \right].$$`
]
]

--

- However, this "abstract saturated model" will have likelihood equal to one, so that deviance is simply
.block[
.small[
`$$D = - 2\text{ Log Likelihood}(\text{Fitted Model}) = - 2 \sum_{i=1}^n \left[y_i \textrm{log}(\hat{\pi}_{1i}) + (1-y_i) \textrm{log}(1-\hat{\pi}_{1i})\right].$$`
]
]

--

- Note that .hlight[deviance is always larger or equal than zero], and will only be zero if the fit is "perfect".

--

- Overall, deviance is a measure of error, so that, .hlight[lower values of deviance means better fit to the data].



---
## Deviance

- Like the metrics used under MLR, it is also often useful to use deviance for a model in relation to another model. We will revisit this soon.

--

- For now, a model we can use for this comparison is the .hlight[null model], that is, the model with only the intercept.

--

- Intuitively, this gives us a sense of how much the model improves from the "worst model", by the addition of  the predictors.

--

- The deviance of the null model, denoted `\(D_0\)`, is thus referred to as the .hlight[null deviance].

--

- To get a general sense of how much better the fitted model is to the null model, compare `\(D\)` to `\(D_0\)`, usually through the difference `\(D_0 - D\)`.

--

- The "larger" this .hlight[change in deviance] `\(D_0 - D\)` is, the more confident we are that the predictors we have included improve model fit.

--

- In large samples, `\(D_0 - D\)` has approximately a chi-squared distribution with degrees of freedom equal to the difference in the number of predictors between the two models.



---
## NBA analysis

For the NBA data for example, we see what looks like a meaningful difference in the two deviance scores.


```r
summary(nbareg)
```

```
## 
## Call:
## glm(formula = win ~ Opp_cent, family = binomial(link = logit), 
##     data = nba)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2760  -0.7073   0.4454   0.7902   1.9593  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  1.13387    0.15145   7.487 7.06e-14
## Opp_cent    -0.12567    0.01655  -7.594 3.11e-14
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 400.05  on 327  degrees of freedom
## Residual deviance: 313.42  on 326  degrees of freedom
## AIC: 317.42
## 
## Number of Fisher Scoring iterations: 5
```



---
## NBA analysis

- We can formalize this by doing a chi-squared test on the null model vs our fitted model. That is,
  
  ```r
  nbareg_null &lt;- glm(win~1,family=binomial(link=logit),data=nba)
  anova(nbareg_null,nbareg,test= "Chisq")
  ```
  
  ```
  ## Analysis of Deviance Table
  ## 
  ## Model 1: win ~ 1
  ## Model 2: win ~ Opp_cent
  ##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)
  ## 1       327     400.05                      
  ## 2       326     313.42  1    86.63 &lt; 2.2e-16
  ```

--

- The low p-value then confirms our previous statement.

--

- We will revisit this again when we look at logistic regression with multiple predictors.

--

- We will be able to use deviance for model comparison and selection by looking at the change in deviance `\(D_{M_1} - D_{M_2}\)`, for two models `\(M_1\)` and `\(M_2\)`, where `\(M_1\)` is nested within `\(M_2\)`.



---
## Confusion matrix

- We can use the estimated probabilities from our fitted model to predict outcomes, and then compare those to the observed values.

--

- For example, we could decide to predict `\(Y=1\)` when the predicted probability exceeds `\(0.5\)` and predict `\(Y=0\)` otherwise.

--

- We then can determine how many cases we classify correctly and incorrectly.

--

- Resulting `\(2 \times 2\)` table is called the .hlight[confusion matrix].

--

- When mis-classification rates are high, model may not be an especially good fit to the data.



---
## Confusion matrix

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt; &lt;/th&gt;
    &lt;th&gt; &lt;/th&gt;
    &lt;th colspan="2"&gt;Observed&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th colspan="2"&gt;&lt;/th&gt;
    &lt;td style="text-align:center"&gt;Y=1&lt;/td&gt;
    &lt;td style="text-align:center"&gt;Y=0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th rowspan="2"&gt;Predicted&lt;/th&gt;
    &lt;td height="50px"&gt;Y=1&lt;/td&gt;
    &lt;td style="text-align:center"&gt;TP (True Positives)&lt;/td&gt;
    &lt;td style="text-align:center"&gt;FP (False Positives)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td height="50px"&gt;Y=0&lt;/td&gt;
    &lt;td style="text-align:center"&gt;FN (False Negatives)&lt;/td&gt;
    &lt;td style="text-align:center"&gt;TN (True Negatives)&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

--

- .hlight[True positive rate (TPR)] = `\(\dfrac{TP}{TP+FN}\)` (also known as .hlight[sensitivity])

--

- .hlight[False negative rate (FNR)] = `\(\dfrac{FN}{TP+FN}\)`

--

- .hlight[True negative rate (TNR)] = `\(\dfrac{TN}{FP+TN}\)` (also known as .hlight[specificity])

--

- .hlight[False positive rate (FPR)] = `\(\dfrac{FP}{FP+TN}\)` (1 - .hlight[specificity])



---
## ROC Curves

- .block[We want high values of sensitivity and low values of (1 - specificity)!]

--

- The .hlight[receiver operating characteristic (ROC)] curve plots 
  + Sensitivity on Y axis
  + 1 - specificity on X axis

--

- Evaluated at lots of different values (beyond 0.5) for the threshold. 

--

- Good fitting logistic regression curves toward the upper left corner, with .hlight[area under the curve (AUC)] near one.

--

- Make ROC curves in R using the pROC package.

--

- By the way, we also often define .hlight[accuracy] as `\(\dfrac{TP + TN}{TP+FN+FP+TN}\)`. This estimates how well the model predicts correctly overall.



---
## NBA analysis

Let's look at the confusion matrix for the NBA data. Load the .hlight[arm], .hlight[e1071], .hlight[caret], and .hlight[pROC] packages.



```r
Conf_mat &lt;- confusionMatrix(as.factor(ifelse(fitted(nbareg) &gt;= 0.5, "W","L")),
                            nba$WINorLOSS,positive = "W")
Conf_mat$table
```

```
##           Reference
## Prediction   L   W
##          L  44  19
##          W  54 211
```

```r
Conf_mat$overall["Accuracy"];
```

```
## Accuracy 
## 0.777439
```

```r
Conf_mat$byClass[c("Sensitivity","Specificity")]
```

```
## Sensitivity Specificity 
##   0.9173913   0.4489796
```

.hlight[confusionMatrix] produces a lot of output. Print the .hlight[Conf_mat] object to see all of them.



---
## NBA analysis



```r
invisible(roc(nba$win,fitted(nbareg),plot=T,print.thres=c(0.3,0.5,0.7),legacy.axes=T,
              print.auc =T,col="red3"))
```

&lt;img src="2-4-logistic-model-assessment_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;


---
## NBA analysis



```r
invisible(roc(nba$win,fitted(nbareg),plot=T,print.thres="best",legacy.axes=T,
              print.auc =T,col="red3"))
```

&lt;img src="2-4-logistic-model-assessment_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;




---

class: center, middle

# What's next? 

### Move on to the readings for the next module!




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
